04/13/2021 09:49:17 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 09:49:17 - INFO - __main__ -   ***** Running training *****
04/13/2021 09:49:17 - INFO - __main__ -     Num examples = 2895
04/13/2021 09:49:17 - INFO - __main__ -     Batch size = 4
04/13/2021 09:49:17 - INFO - __main__ -     Num steps = 14460
04/13/2021 09:49:17 - INFO - __main__ -   ***** Running validations *****
04/13/2021 09:49:17 - INFO - __main__ -     Num orig examples = 150
04/13/2021 09:49:17 - INFO - __main__ -     Num split examples = 150
04/13/2021 09:49:17 - INFO - __main__ -     Batch size = 4
04/13/2021 09:49:17 - INFO - modeling -   loading archive file pt_model/laptop_pt/
04/13/2021 09:49:17 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/13/2021 09:49:20 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/13/2021 09:49:20 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/13/2021 09:57:36 - INFO - __main__ -   validation loss: 36.582776, epoch: 1
04/13/2021 09:57:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 09:57:37 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 09:57:37 - INFO - __main__ -     Num examples = 150
04/13/2021 09:57:37 - INFO - __main__ -     Batch size = 8
04/13/2021 10:05:50 - INFO - __main__ -   validation loss: 34.238257, epoch: 2
04/13/2021 10:05:51 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:05:51 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:05:51 - INFO - __main__ -     Num examples = 150
04/13/2021 10:05:51 - INFO - __main__ -     Batch size = 8
04/13/2021 10:14:04 - INFO - __main__ -   validation loss: 41.149499, epoch: 3
04/13/2021 10:14:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:14:05 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:14:05 - INFO - __main__ -     Num examples = 150
04/13/2021 10:14:05 - INFO - __main__ -     Batch size = 8
04/13/2021 10:22:12 - INFO - __main__ -   validation loss: 61.273733, epoch: 4
04/13/2021 10:22:13 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:22:14 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:22:14 - INFO - __main__ -     Num examples = 150
04/13/2021 10:22:14 - INFO - __main__ -     Batch size = 8
04/13/2021 10:30:20 - INFO - __main__ -   validation loss: 74.143296, epoch: 5
04/13/2021 10:30:21 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:30:21 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:30:21 - INFO - __main__ -     Num examples = 150
04/13/2021 10:30:21 - INFO - __main__ -     Batch size = 8
04/13/2021 10:38:27 - INFO - __main__ -   validation loss: 78.359261, epoch: 6
04/13/2021 10:38:28 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:38:28 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:38:28 - INFO - __main__ -     Num examples = 150
04/13/2021 10:38:28 - INFO - __main__ -     Batch size = 8
04/13/2021 10:46:36 - INFO - __main__ -   validation loss: 75.496499, epoch: 7
04/13/2021 10:46:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:46:37 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:46:37 - INFO - __main__ -     Num examples = 150
04/13/2021 10:46:37 - INFO - __main__ -     Batch size = 8
04/13/2021 10:54:43 - INFO - __main__ -   validation loss: 67.489082, epoch: 8
04/13/2021 10:54:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 10:54:44 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 10:54:44 - INFO - __main__ -     Num examples = 150
04/13/2021 10:54:44 - INFO - __main__ -     Batch size = 8
04/13/2021 11:02:49 - INFO - __main__ -   validation loss: 65.859666, epoch: 9
04/13/2021 11:02:51 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:02:51 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:02:51 - INFO - __main__ -     Num examples = 150
04/13/2021 11:02:51 - INFO - __main__ -     Batch size = 8
04/13/2021 11:10:58 - INFO - __main__ -   validation loss: 63.222324, epoch: 10
04/13/2021 11:10:59 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:10:59 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:10:59 - INFO - __main__ -     Num examples = 150
04/13/2021 11:10:59 - INFO - __main__ -     Batch size = 8
04/13/2021 11:19:05 - INFO - __main__ -   validation loss: 68.062292, epoch: 11
04/13/2021 11:19:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:19:06 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:19:06 - INFO - __main__ -     Num examples = 150
04/13/2021 11:19:06 - INFO - __main__ -     Batch size = 8
04/13/2021 11:27:10 - INFO - __main__ -   validation loss: 69.833757, epoch: 12
04/13/2021 11:27:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:27:11 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:27:11 - INFO - __main__ -     Num examples = 150
04/13/2021 11:27:11 - INFO - __main__ -     Batch size = 8
04/13/2021 11:35:18 - INFO - __main__ -   validation loss: 68.273890, epoch: 13
04/13/2021 11:35:19 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:35:19 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:35:19 - INFO - __main__ -     Num examples = 150
04/13/2021 11:35:19 - INFO - __main__ -     Batch size = 8
04/13/2021 11:43:27 - INFO - __main__ -   validation loss: 67.525339, epoch: 14
04/13/2021 11:43:29 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:43:30 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:43:30 - INFO - __main__ -     Num examples = 150
04/13/2021 11:43:30 - INFO - __main__ -     Batch size = 8
04/13/2021 11:51:38 - INFO - __main__ -   validation loss: 68.685407, epoch: 15
04/13/2021 11:51:39 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:51:39 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:51:39 - INFO - __main__ -     Num examples = 150
04/13/2021 11:51:39 - INFO - __main__ -     Batch size = 8
04/13/2021 11:59:45 - INFO - __main__ -   validation loss: 68.200068, epoch: 16
04/13/2021 11:59:46 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 11:59:46 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 11:59:46 - INFO - __main__ -     Num examples = 150
04/13/2021 11:59:46 - INFO - __main__ -     Batch size = 8
04/13/2021 12:07:55 - INFO - __main__ -   validation loss: 64.490609, epoch: 17
04/13/2021 12:07:56 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 12:07:56 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 12:07:56 - INFO - __main__ -     Num examples = 150
04/13/2021 12:07:56 - INFO - __main__ -     Batch size = 8
04/13/2021 12:16:04 - INFO - __main__ -   validation loss: 65.168245, epoch: 18
04/13/2021 12:16:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 12:16:05 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 12:16:05 - INFO - __main__ -     Num examples = 150
04/13/2021 12:16:05 - INFO - __main__ -     Batch size = 8
04/13/2021 12:24:10 - INFO - __main__ -   validation loss: 65.233454, epoch: 19
04/13/2021 12:24:12 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 12:24:12 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 12:24:12 - INFO - __main__ -     Num examples = 150
04/13/2021 12:24:12 - INFO - __main__ -     Batch size = 8
04/13/2021 12:31:59 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:00 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:00 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:01 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:02 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:02 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:03 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:04 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:04 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:05 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:06 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:06 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:07 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:08 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:08 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:09 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:10 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:11 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:11 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 12:32:18 - INFO - __main__ -   validation loss: 65.174251, epoch: 20
04/13/2021 12:32:19 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 12:32:19 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 12:32:19 - INFO - __main__ -     Num examples = 150
04/13/2021 12:32:19 - INFO - __main__ -     Batch size = 8
04/13/2021 12:32:24 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/laptop_pt/vocab.txt
04/13/2021 12:32:24 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 12:32:24 - INFO - __main__ -     Num examples = 800
04/13/2021 12:32:24 - INFO - __main__ -     Batch size = 8

04/12/2021 09:42:48 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 09:42:48 - INFO - __main__ -   ***** Running training *****
04/12/2021 09:42:48 - INFO - __main__ -     Num examples = 1850
04/12/2021 09:42:48 - INFO - __main__ -     Batch size = 4
04/12/2021 09:42:48 - INFO - __main__ -     Num steps = 1848
04/12/2021 09:42:48 - INFO - __main__ -   ***** Running validations *****
04/12/2021 09:42:48 - INFO - __main__ -     Num orig examples = 150
04/12/2021 09:42:48 - INFO - __main__ -     Num split examples = 150
04/12/2021 09:42:48 - INFO - __main__ -     Batch size = 4
04/12/2021 09:42:48 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 09:42:48 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 09:42:50 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 09:42:50 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 09:48:07 - INFO - __main__ -   validation loss: 20.856821, epoch: 1
04/12/2021 09:48:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 09:48:09 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 09:48:09 - INFO - __main__ -     Num examples = 150
04/12/2021 09:48:09 - INFO - __main__ -     Batch size = 8
04/12/2021 09:53:26 - INFO - __main__ -   validation loss: 26.851084, epoch: 2
04/12/2021 09:53:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 09:53:27 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 09:53:27 - INFO - __main__ -     Num examples = 150
04/12/2021 09:53:27 - INFO - __main__ -     Batch size = 8
04/12/2021 09:58:43 - INFO - __main__ -   validation loss: 36.199765, epoch: 3
04/12/2021 09:58:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 09:58:45 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 09:58:45 - INFO - __main__ -     Num examples = 150
04/12/2021 09:58:45 - INFO - __main__ -     Batch size = 8
04/12/2021 10:03:52 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:03:53 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:03:53 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:03:59 - INFO - __main__ -   validation loss: 38.639448, epoch: 4
04/12/2021 10:04:01 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:04:01 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:04:01 - INFO - __main__ -     Num examples = 150
04/12/2021 10:04:01 - INFO - __main__ -     Batch size = 8
04/12/2021 10:04:04 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:04:04 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:04:04 - INFO - __main__ -     Num examples = 676
04/12/2021 10:04:04 - INFO - __main__ -     Batch size = 8
04/12/2021 10:04:15 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:04:15 - INFO - __main__ -   ***** Running training *****
04/12/2021 10:04:15 - INFO - __main__ -     Num examples = 1850
04/12/2021 10:04:15 - INFO - __main__ -     Batch size = 4
04/12/2021 10:04:15 - INFO - __main__ -     Num steps = 1848
04/12/2021 10:04:15 - INFO - __main__ -   ***** Running validations *****
04/12/2021 10:04:15 - INFO - __main__ -     Num orig examples = 150
04/12/2021 10:04:15 - INFO - __main__ -     Num split examples = 150
04/12/2021 10:04:15 - INFO - __main__ -     Batch size = 4
04/12/2021 10:04:15 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 10:04:15 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 10:04:17 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 10:04:17 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 10:09:31 - INFO - __main__ -   validation loss: 21.423678, epoch: 1
04/12/2021 10:09:33 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:09:33 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:09:33 - INFO - __main__ -     Num examples = 150
04/12/2021 10:09:33 - INFO - __main__ -     Batch size = 8
04/12/2021 10:14:48 - INFO - __main__ -   validation loss: 27.908364, epoch: 2
04/12/2021 10:14:49 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:14:49 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:14:49 - INFO - __main__ -     Num examples = 150
04/12/2021 10:14:49 - INFO - __main__ -     Batch size = 8
04/12/2021 10:20:04 - INFO - __main__ -   validation loss: 37.196616, epoch: 3
04/12/2021 10:20:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:20:05 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:20:05 - INFO - __main__ -     Num examples = 150
04/12/2021 10:20:05 - INFO - __main__ -     Batch size = 8
04/12/2021 10:25:11 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:25:12 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:25:13 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:25:19 - INFO - __main__ -   validation loss: 41.217378, epoch: 4
04/12/2021 10:25:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:25:20 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:25:20 - INFO - __main__ -     Num examples = 150
04/12/2021 10:25:20 - INFO - __main__ -     Batch size = 8
04/12/2021 10:25:24 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:25:24 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:25:24 - INFO - __main__ -     Num examples = 676
04/12/2021 10:25:24 - INFO - __main__ -     Batch size = 8
04/12/2021 10:25:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:25:35 - INFO - __main__ -   ***** Running training *****
04/12/2021 10:25:35 - INFO - __main__ -     Num examples = 1850
04/12/2021 10:25:35 - INFO - __main__ -     Batch size = 4
04/12/2021 10:25:35 - INFO - __main__ -     Num steps = 1848
04/12/2021 10:25:35 - INFO - __main__ -   ***** Running validations *****
04/12/2021 10:25:35 - INFO - __main__ -     Num orig examples = 150
04/12/2021 10:25:35 - INFO - __main__ -     Num split examples = 150
04/12/2021 10:25:35 - INFO - __main__ -     Batch size = 4
04/12/2021 10:25:35 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 10:25:35 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 10:25:37 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 10:25:37 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 10:30:52 - INFO - __main__ -   validation loss: 18.159448, epoch: 1
04/12/2021 10:30:53 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:30:53 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:30:53 - INFO - __main__ -     Num examples = 150
04/12/2021 10:30:53 - INFO - __main__ -     Batch size = 8
04/12/2021 10:36:09 - INFO - __main__ -   validation loss: 23.395862, epoch: 2
04/12/2021 10:36:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:36:10 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:36:10 - INFO - __main__ -     Num examples = 150
04/12/2021 10:36:10 - INFO - __main__ -     Batch size = 8
04/12/2021 10:41:25 - INFO - __main__ -   validation loss: 33.361953, epoch: 3
04/12/2021 10:41:26 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:41:26 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:41:26 - INFO - __main__ -     Num examples = 150
04/12/2021 10:41:26 - INFO - __main__ -     Batch size = 8
04/12/2021 10:46:32 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:46:33 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:46:34 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 10:46:40 - INFO - __main__ -   validation loss: 35.547815, epoch: 4
04/12/2021 10:46:41 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:46:41 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:46:41 - INFO - __main__ -     Num examples = 150
04/12/2021 10:46:41 - INFO - __main__ -     Batch size = 8
04/12/2021 10:46:45 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:46:45 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:46:45 - INFO - __main__ -     Num examples = 676
04/12/2021 10:46:45 - INFO - __main__ -     Batch size = 8
04/12/2021 10:46:55 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:46:55 - INFO - __main__ -   ***** Running training *****
04/12/2021 10:46:55 - INFO - __main__ -     Num examples = 1850
04/12/2021 10:46:55 - INFO - __main__ -     Batch size = 4
04/12/2021 10:46:55 - INFO - __main__ -     Num steps = 1848
04/12/2021 10:46:55 - INFO - __main__ -   ***** Running validations *****
04/12/2021 10:46:55 - INFO - __main__ -     Num orig examples = 150
04/12/2021 10:46:55 - INFO - __main__ -     Num split examples = 150
04/12/2021 10:46:55 - INFO - __main__ -     Batch size = 4
04/12/2021 10:46:55 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 10:46:55 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 10:46:57 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 10:46:57 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 10:52:13 - INFO - __main__ -   validation loss: 19.291845, epoch: 1
04/12/2021 10:52:14 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:52:14 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:52:14 - INFO - __main__ -     Num examples = 150
04/12/2021 10:52:14 - INFO - __main__ -     Batch size = 8
04/12/2021 10:57:30 - INFO - __main__ -   validation loss: 23.080481, epoch: 2
04/12/2021 10:57:31 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 10:57:31 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 10:57:31 - INFO - __main__ -     Num examples = 150
04/12/2021 10:57:31 - INFO - __main__ -     Batch size = 8
04/12/2021 11:02:46 - INFO - __main__ -   validation loss: 33.314743, epoch: 3
04/12/2021 11:02:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:02:47 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:02:47 - INFO - __main__ -     Num examples = 150
04/12/2021 11:02:47 - INFO - __main__ -     Batch size = 8
04/12/2021 11:07:56 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:07:57 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:07:58 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:08:04 - INFO - __main__ -   validation loss: 36.955359, epoch: 4
04/12/2021 11:08:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:08:05 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:08:05 - INFO - __main__ -     Num examples = 150
04/12/2021 11:08:05 - INFO - __main__ -     Batch size = 8
04/12/2021 11:08:09 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:08:09 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:08:09 - INFO - __main__ -     Num examples = 676
04/12/2021 11:08:09 - INFO - __main__ -     Batch size = 8
04/12/2021 11:08:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:08:20 - INFO - __main__ -   ***** Running training *****
04/12/2021 11:08:20 - INFO - __main__ -     Num examples = 1850
04/12/2021 11:08:20 - INFO - __main__ -     Batch size = 4
04/12/2021 11:08:20 - INFO - __main__ -     Num steps = 1848
04/12/2021 11:08:20 - INFO - __main__ -   ***** Running validations *****
04/12/2021 11:08:20 - INFO - __main__ -     Num orig examples = 150
04/12/2021 11:08:20 - INFO - __main__ -     Num split examples = 150
04/12/2021 11:08:20 - INFO - __main__ -     Batch size = 4
04/12/2021 11:08:20 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 11:08:20 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 11:08:22 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 11:08:22 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 11:13:37 - INFO - __main__ -   validation loss: 21.453520, epoch: 1
04/12/2021 11:13:38 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:13:38 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:13:38 - INFO - __main__ -     Num examples = 150
04/12/2021 11:13:38 - INFO - __main__ -     Batch size = 8
04/12/2021 11:18:53 - INFO - __main__ -   validation loss: 35.557890, epoch: 2
04/12/2021 11:18:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:18:54 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:18:54 - INFO - __main__ -     Num examples = 150
04/12/2021 11:18:54 - INFO - __main__ -     Batch size = 8
04/12/2021 11:24:08 - INFO - __main__ -   validation loss: 42.201215, epoch: 3
04/12/2021 11:24:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:24:10 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:24:10 - INFO - __main__ -     Num examples = 150
04/12/2021 11:24:10 - INFO - __main__ -     Batch size = 8
04/12/2021 11:29:16 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:29:16 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:29:17 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:29:23 - INFO - __main__ -   validation loss: 44.341048, epoch: 4
04/12/2021 11:29:24 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:29:24 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:29:24 - INFO - __main__ -     Num examples = 150
04/12/2021 11:29:24 - INFO - __main__ -     Batch size = 8
04/12/2021 11:29:28 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:29:28 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:29:28 - INFO - __main__ -     Num examples = 676
04/12/2021 11:29:28 - INFO - __main__ -     Batch size = 8
04/12/2021 11:29:39 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:29:39 - INFO - __main__ -   ***** Running training *****
04/12/2021 11:29:39 - INFO - __main__ -     Num examples = 1850
04/12/2021 11:29:39 - INFO - __main__ -     Batch size = 4
04/12/2021 11:29:39 - INFO - __main__ -     Num steps = 1848
04/12/2021 11:29:39 - INFO - __main__ -   ***** Running validations *****
04/12/2021 11:29:39 - INFO - __main__ -     Num orig examples = 150
04/12/2021 11:29:39 - INFO - __main__ -     Num split examples = 150
04/12/2021 11:29:39 - INFO - __main__ -     Batch size = 4
04/12/2021 11:29:39 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 11:29:39 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 11:29:41 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 11:29:41 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 11:34:57 - INFO - __main__ -   validation loss: 31.080762, epoch: 1
04/12/2021 11:34:58 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:34:58 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:34:58 - INFO - __main__ -     Num examples = 150
04/12/2021 11:34:58 - INFO - __main__ -     Batch size = 8
04/12/2021 11:40:14 - INFO - __main__ -   validation loss: 25.915990, epoch: 2
04/12/2021 11:40:15 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:40:16 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:40:16 - INFO - __main__ -     Num examples = 150
04/12/2021 11:40:16 - INFO - __main__ -     Batch size = 8
04/12/2021 11:45:34 - INFO - __main__ -   validation loss: 33.836545, epoch: 3
04/12/2021 11:45:35 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:45:35 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:45:35 - INFO - __main__ -     Num examples = 150
04/12/2021 11:45:35 - INFO - __main__ -     Batch size = 8
04/12/2021 11:50:45 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:50:45 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:50:46 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 11:50:52 - INFO - __main__ -   validation loss: 34.705687, epoch: 4
04/12/2021 11:50:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:50:54 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:50:54 - INFO - __main__ -     Num examples = 150
04/12/2021 11:50:54 - INFO - __main__ -     Batch size = 8
04/12/2021 11:50:58 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:50:58 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:50:58 - INFO - __main__ -     Num examples = 676
04/12/2021 11:50:58 - INFO - __main__ -     Batch size = 8
04/12/2021 11:51:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:51:09 - INFO - __main__ -   ***** Running training *****
04/12/2021 11:51:09 - INFO - __main__ -     Num examples = 1850
04/12/2021 11:51:09 - INFO - __main__ -     Batch size = 4
04/12/2021 11:51:09 - INFO - __main__ -     Num steps = 1848
04/12/2021 11:51:09 - INFO - __main__ -   ***** Running validations *****
04/12/2021 11:51:09 - INFO - __main__ -     Num orig examples = 150
04/12/2021 11:51:09 - INFO - __main__ -     Num split examples = 150
04/12/2021 11:51:09 - INFO - __main__ -     Batch size = 4
04/12/2021 11:51:09 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 11:51:09 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 11:51:11 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 11:51:11 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 11:56:26 - INFO - __main__ -   validation loss: 19.798658, epoch: 1
04/12/2021 11:56:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 11:56:27 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 11:56:27 - INFO - __main__ -     Num examples = 150
04/12/2021 11:56:27 - INFO - __main__ -     Batch size = 8
04/12/2021 12:01:42 - INFO - __main__ -   validation loss: 20.465927, epoch: 2
04/12/2021 12:01:43 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:01:43 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:01:43 - INFO - __main__ -     Num examples = 150
04/12/2021 12:01:43 - INFO - __main__ -     Batch size = 8
04/12/2021 12:06:57 - INFO - __main__ -   validation loss: 31.373787, epoch: 3
04/12/2021 12:06:58 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:06:58 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:06:58 - INFO - __main__ -     Num examples = 150
04/12/2021 12:06:58 - INFO - __main__ -     Batch size = 8
04/12/2021 12:12:04 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:12:05 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:12:06 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:12:12 - INFO - __main__ -   validation loss: 33.526367, epoch: 4
04/12/2021 12:12:13 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:12:13 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:12:13 - INFO - __main__ -     Num examples = 150
04/12/2021 12:12:13 - INFO - __main__ -     Batch size = 8
04/12/2021 12:12:17 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:12:17 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:12:17 - INFO - __main__ -     Num examples = 676
04/12/2021 12:12:17 - INFO - __main__ -     Batch size = 8
04/12/2021 12:12:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:12:28 - INFO - __main__ -   ***** Running training *****
04/12/2021 12:12:28 - INFO - __main__ -     Num examples = 1850
04/12/2021 12:12:28 - INFO - __main__ -     Batch size = 4
04/12/2021 12:12:28 - INFO - __main__ -     Num steps = 1848
04/12/2021 12:12:28 - INFO - __main__ -   ***** Running validations *****
04/12/2021 12:12:28 - INFO - __main__ -     Num orig examples = 150
04/12/2021 12:12:28 - INFO - __main__ -     Num split examples = 150
04/12/2021 12:12:28 - INFO - __main__ -     Batch size = 4
04/12/2021 12:12:28 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 12:12:28 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 12:12:30 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 12:12:30 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 12:17:45 - INFO - __main__ -   validation loss: 16.338598, epoch: 1
04/12/2021 12:17:46 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:17:46 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:17:46 - INFO - __main__ -     Num examples = 150
04/12/2021 12:17:46 - INFO - __main__ -     Batch size = 8
04/12/2021 12:23:02 - INFO - __main__ -   validation loss: 24.814896, epoch: 2
04/12/2021 12:23:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:23:06 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:23:06 - INFO - __main__ -     Num examples = 150
04/12/2021 12:23:06 - INFO - __main__ -     Batch size = 8
04/12/2021 12:28:22 - INFO - __main__ -   validation loss: 37.077437, epoch: 3
04/12/2021 12:28:23 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:28:23 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:28:23 - INFO - __main__ -     Num examples = 150
04/12/2021 12:28:23 - INFO - __main__ -     Batch size = 8
04/12/2021 12:33:31 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:33:31 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:33:32 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:33:38 - INFO - __main__ -   validation loss: 39.075640, epoch: 4
04/12/2021 12:33:40 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:33:40 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:33:40 - INFO - __main__ -     Num examples = 150
04/12/2021 12:33:40 - INFO - __main__ -     Batch size = 8
04/12/2021 12:33:43 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:33:43 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:33:43 - INFO - __main__ -     Num examples = 676
04/12/2021 12:33:43 - INFO - __main__ -     Batch size = 8
04/12/2021 12:33:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:33:54 - INFO - __main__ -   ***** Running training *****
04/12/2021 12:33:54 - INFO - __main__ -     Num examples = 1850
04/12/2021 12:33:54 - INFO - __main__ -     Batch size = 4
04/12/2021 12:33:54 - INFO - __main__ -     Num steps = 1848
04/12/2021 12:33:54 - INFO - __main__ -   ***** Running validations *****
04/12/2021 12:33:54 - INFO - __main__ -     Num orig examples = 150
04/12/2021 12:33:54 - INFO - __main__ -     Num split examples = 150
04/12/2021 12:33:54 - INFO - __main__ -     Batch size = 4
04/12/2021 12:33:54 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/12/2021 12:33:54 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/12/2021 12:33:56 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/12/2021 12:33:56 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/12/2021 12:39:10 - INFO - __main__ -   validation loss: 25.172645, epoch: 1
04/12/2021 12:39:11 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:39:11 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:39:11 - INFO - __main__ -     Num examples = 150
04/12/2021 12:39:11 - INFO - __main__ -     Batch size = 8
04/12/2021 12:44:27 - INFO - __main__ -   validation loss: 23.944504, epoch: 2
04/12/2021 12:44:28 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:44:28 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:44:28 - INFO - __main__ -     Num examples = 150
04/12/2021 12:44:28 - INFO - __main__ -     Batch size = 8
04/12/2021 12:49:42 - INFO - __main__ -   validation loss: 31.005342, epoch: 3
04/12/2021 12:49:44 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:49:44 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:49:44 - INFO - __main__ -     Num examples = 150
04/12/2021 12:49:44 - INFO - __main__ -     Batch size = 8
04/12/2021 12:54:50 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:54:51 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:54:51 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/12/2021 12:54:58 - INFO - __main__ -   validation loss: 32.820950, epoch: 4
04/12/2021 12:54:59 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:54:59 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:54:59 - INFO - __main__ -     Num examples = 150
04/12/2021 12:54:59 - INFO - __main__ -     Batch size = 8
04/12/2021 12:55:02 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/12/2021 12:55:02 - INFO - __main__ -   ***** Running evaluation *****
04/12/2021 12:55:02 - INFO - __main__ -     Num examples = 676
04/12/2021 12:55:02 - INFO - __main__ -     Batch size = 8

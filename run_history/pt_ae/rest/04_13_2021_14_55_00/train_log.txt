04/13/2021 12:59:31 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 12:59:31 - INFO - __main__ -   ***** Running training *****
04/13/2021 12:59:31 - INFO - __main__ -     Num examples = 1850
04/13/2021 12:59:31 - INFO - __main__ -     Batch size = 4
04/13/2021 12:59:31 - INFO - __main__ -     Num steps = 9240
04/13/2021 12:59:31 - INFO - __main__ -   ***** Running validations *****
04/13/2021 12:59:31 - INFO - __main__ -     Num orig examples = 150
04/13/2021 12:59:31 - INFO - __main__ -     Num split examples = 150
04/13/2021 12:59:31 - INFO - __main__ -     Batch size = 4
04/13/2021 12:59:31 - INFO - modeling -   loading archive file pt_model/rest_pt/
04/13/2021 12:59:31 - INFO - modeling -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

04/13/2021 12:59:34 - INFO - modeling -   Weights of BertForABSA not initialized from pretrained model: ['groie.pre_layers.0.attention.self.query.weight', 'groie.pre_layers.0.attention.self.query.bias', 'groie.pre_layers.0.attention.self.key.weight', 'groie.pre_layers.0.attention.self.key.bias', 'groie.pre_layers.0.attention.self.value.weight', 'groie.pre_layers.0.attention.self.value.bias', 'groie.pre_layers.0.attention.output.dense.weight', 'groie.pre_layers.0.attention.output.dense.bias', 'groie.pre_layers.0.attention.output.LayerNorm.weight', 'groie.pre_layers.0.attention.output.LayerNorm.bias', 'groie.pre_layers.0.intermediate.dense.weight', 'groie.pre_layers.0.intermediate.dense.bias', 'groie.pre_layers.0.output.dense.weight', 'groie.pre_layers.0.output.dense.bias', 'groie.pre_layers.0.output.LayerNorm.weight', 'groie.pre_layers.0.output.LayerNorm.bias', 'groie.pre_layers.1.attention.self.query.weight', 'groie.pre_layers.1.attention.self.query.bias', 'groie.pre_layers.1.attention.self.key.weight', 'groie.pre_layers.1.attention.self.key.bias', 'groie.pre_layers.1.attention.self.value.weight', 'groie.pre_layers.1.attention.self.value.bias', 'groie.pre_layers.1.attention.output.dense.weight', 'groie.pre_layers.1.attention.output.dense.bias', 'groie.pre_layers.1.attention.output.LayerNorm.weight', 'groie.pre_layers.1.attention.output.LayerNorm.bias', 'groie.pre_layers.1.intermediate.dense.weight', 'groie.pre_layers.1.intermediate.dense.bias', 'groie.pre_layers.1.output.dense.weight', 'groie.pre_layers.1.output.dense.bias', 'groie.pre_layers.1.output.LayerNorm.weight', 'groie.pre_layers.1.output.LayerNorm.bias', 'groie.pre_layers.2.attention.self.query.weight', 'groie.pre_layers.2.attention.self.query.bias', 'groie.pre_layers.2.attention.self.key.weight', 'groie.pre_layers.2.attention.self.key.bias', 'groie.pre_layers.2.attention.self.value.weight', 'groie.pre_layers.2.attention.self.value.bias', 'groie.pre_layers.2.attention.output.dense.weight', 'groie.pre_layers.2.attention.output.dense.bias', 'groie.pre_layers.2.attention.output.LayerNorm.weight', 'groie.pre_layers.2.attention.output.LayerNorm.bias', 'groie.pre_layers.2.intermediate.dense.weight', 'groie.pre_layers.2.intermediate.dense.bias', 'groie.pre_layers.2.output.dense.weight', 'groie.pre_layers.2.output.dense.bias', 'groie.pre_layers.2.output.LayerNorm.weight', 'groie.pre_layers.2.output.LayerNorm.bias', 'groie.pre_layers.3.attention.self.query.weight', 'groie.pre_layers.3.attention.self.query.bias', 'groie.pre_layers.3.attention.self.key.weight', 'groie.pre_layers.3.attention.self.key.bias', 'groie.pre_layers.3.attention.self.value.weight', 'groie.pre_layers.3.attention.self.value.bias', 'groie.pre_layers.3.attention.output.dense.weight', 'groie.pre_layers.3.attention.output.dense.bias', 'groie.pre_layers.3.attention.output.LayerNorm.weight', 'groie.pre_layers.3.attention.output.LayerNorm.bias', 'groie.pre_layers.3.intermediate.dense.weight', 'groie.pre_layers.3.intermediate.dense.bias', 'groie.pre_layers.3.output.dense.weight', 'groie.pre_layers.3.output.dense.bias', 'groie.pre_layers.3.output.LayerNorm.weight', 'groie.pre_layers.3.output.LayerNorm.bias', 'groie.crf_layers.0.start_transitions', 'groie.crf_layers.0.end_transitions', 'groie.crf_layers.0.transitions', 'groie.crf_layers.1.start_transitions', 'groie.crf_layers.1.end_transitions', 'groie.crf_layers.1.transitions', 'groie.crf_layers.2.start_transitions', 'groie.crf_layers.2.end_transitions', 'groie.crf_layers.2.transitions', 'groie.crf_layers.3.start_transitions', 'groie.crf_layers.3.end_transitions', 'groie.crf_layers.3.transitions', 'groie.classifier.weight', 'groie.classifier.bias']
04/13/2021 12:59:34 - INFO - modeling -   Weights from pretrained model not used in BertForABSA: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'qa_outputs.weight', 'qa_outputs.bias']
04/13/2021 13:05:14 - INFO - __main__ -   validation loss: 29.119104, epoch: 1
04/13/2021 13:05:16 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:05:16 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:05:16 - INFO - __main__ -     Num examples = 150
04/13/2021 13:05:16 - INFO - __main__ -     Batch size = 8
04/13/2021 13:10:46 - INFO - __main__ -   validation loss: 33.493583, epoch: 2
04/13/2021 13:10:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:10:47 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:10:47 - INFO - __main__ -     Num examples = 150
04/13/2021 13:10:47 - INFO - __main__ -     Batch size = 8
04/13/2021 13:16:19 - INFO - __main__ -   validation loss: 48.699624, epoch: 3
04/13/2021 13:16:21 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:16:21 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:16:21 - INFO - __main__ -     Num examples = 150
04/13/2021 13:16:21 - INFO - __main__ -     Batch size = 8
04/13/2021 13:21:49 - INFO - __main__ -   validation loss: 46.640103, epoch: 4
04/13/2021 13:21:50 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:21:50 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:21:50 - INFO - __main__ -     Num examples = 150
04/13/2021 13:21:50 - INFO - __main__ -     Batch size = 8
04/13/2021 13:27:25 - INFO - __main__ -   validation loss: 63.204888, epoch: 5
04/13/2021 13:27:27 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:27:27 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:27:27 - INFO - __main__ -     Num examples = 150
04/13/2021 13:27:27 - INFO - __main__ -     Batch size = 8
04/13/2021 13:33:06 - INFO - __main__ -   validation loss: 70.293108, epoch: 6
04/13/2021 13:33:08 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:33:08 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:33:08 - INFO - __main__ -     Num examples = 150
04/13/2021 13:33:08 - INFO - __main__ -     Batch size = 8
04/13/2021 13:38:35 - INFO - __main__ -   validation loss: 79.493088, epoch: 7
04/13/2021 13:38:37 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:38:37 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:38:37 - INFO - __main__ -     Num examples = 150
04/13/2021 13:38:37 - INFO - __main__ -     Batch size = 8
04/13/2021 13:44:20 - INFO - __main__ -   validation loss: 77.711135, epoch: 8
04/13/2021 13:44:22 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:44:22 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:44:22 - INFO - __main__ -     Num examples = 150
04/13/2021 13:44:22 - INFO - __main__ -     Batch size = 8
04/13/2021 13:50:05 - INFO - __main__ -   validation loss: 84.136940, epoch: 9
04/13/2021 13:50:06 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:50:06 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:50:06 - INFO - __main__ -     Num examples = 150
04/13/2021 13:50:06 - INFO - __main__ -     Batch size = 8
04/13/2021 13:55:31 - INFO - __main__ -   validation loss: 65.362834, epoch: 10
04/13/2021 13:55:33 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 13:55:33 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 13:55:33 - INFO - __main__ -     Num examples = 150
04/13/2021 13:55:33 - INFO - __main__ -     Batch size = 8
04/13/2021 14:00:49 - INFO - __main__ -   validation loss: 67.465783, epoch: 11
04/13/2021 14:00:51 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:00:51 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:00:51 - INFO - __main__ -     Num examples = 150
04/13/2021 14:00:51 - INFO - __main__ -     Batch size = 8
04/13/2021 14:06:29 - INFO - __main__ -   validation loss: 68.980308, epoch: 12
04/13/2021 14:06:30 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:06:30 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:06:30 - INFO - __main__ -     Num examples = 150
04/13/2021 14:06:30 - INFO - __main__ -     Batch size = 8
04/13/2021 14:11:59 - INFO - __main__ -   validation loss: 68.167572, epoch: 13
04/13/2021 14:12:00 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:12:00 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:12:00 - INFO - __main__ -     Num examples = 150
04/13/2021 14:12:00 - INFO - __main__ -     Batch size = 8
04/13/2021 14:17:55 - INFO - __main__ -   validation loss: 67.358242, epoch: 14
04/13/2021 14:17:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:17:57 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:17:57 - INFO - __main__ -     Num examples = 150
04/13/2021 14:17:57 - INFO - __main__ -     Batch size = 8
04/13/2021 14:23:55 - INFO - __main__ -   validation loss: 66.895334, epoch: 15
04/13/2021 14:23:57 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:23:57 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:23:57 - INFO - __main__ -     Num examples = 150
04/13/2021 14:23:57 - INFO - __main__ -     Batch size = 8
04/13/2021 14:29:46 - INFO - __main__ -   validation loss: 66.002223, epoch: 16
04/13/2021 14:29:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:29:47 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:29:47 - INFO - __main__ -     Num examples = 150
04/13/2021 14:29:47 - INFO - __main__ -     Batch size = 8
04/13/2021 14:35:40 - INFO - __main__ -   validation loss: 65.461846, epoch: 17
04/13/2021 14:35:42 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:35:42 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:35:42 - INFO - __main__ -     Num examples = 150
04/13/2021 14:35:42 - INFO - __main__ -     Batch size = 8
04/13/2021 14:41:46 - INFO - __main__ -   validation loss: 65.713075, epoch: 18
04/13/2021 14:41:47 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:41:47 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:41:47 - INFO - __main__ -     Num examples = 150
04/13/2021 14:41:47 - INFO - __main__ -     Batch size = 8
04/13/2021 14:47:35 - INFO - __main__ -   validation loss: 65.537196, epoch: 19
04/13/2021 14:47:36 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:47:36 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:47:36 - INFO - __main__ -     Num examples = 150
04/13/2021 14:47:36 - INFO - __main__ -     Batch size = 8
04/13/2021 14:53:26 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:27 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:27 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:28 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:29 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:30 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:30 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:31 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:32 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:33 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:33 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:34 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:35 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:36 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:36 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:37 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:38 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:39 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:39 - WARNING - optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.
04/13/2021 14:53:46 - INFO - __main__ -   validation loss: 65.498592, epoch: 20
04/13/2021 14:53:48 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:53:48 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:53:48 - INFO - __main__ -     Num examples = 150
04/13/2021 14:53:48 - INFO - __main__ -     Batch size = 8
04/13/2021 14:53:52 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file pt_model/rest_pt/vocab.txt
04/13/2021 14:53:52 - INFO - __main__ -   ***** Running evaluation *****
04/13/2021 14:53:52 - INFO - __main__ -     Num examples = 676
04/13/2021 14:53:52 - INFO - __main__ -     Batch size = 8
